{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40f910333baa39f",
   "metadata": {},
   "source": [
    "# Model: XGBoost (final)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3d0bd208419a611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T11:04:24.691412Z",
     "iopub.status.busy": "2025-12-28T11:04:24.691154Z",
     "iopub.status.idle": "2025-12-28T11:04:26.391871Z",
     "shell.execute_reply": "2025-12-28T11:04:26.391500Z"
    },
    "ExecuteTime": {
     "end_time": "2025-12-28T11:06:57.877097Z",
     "start_time": "2025-12-28T11:06:56.518396Z"
    }
   },
   "source": [
    "# Train the final XGBoost model using the Best Params provided by user.\n",
    "# Robust behavior:\n",
    "#  - Prefer `X_train_fs`/`X_test_fs` and `y_train_model`/`y_test_model` if available in notebook globals.\n",
    "#  - Otherwise load `../data/dataset_final_processed.csv`, prepare train/test (stratified split), clean and impute, run SelectKBest(k=8) if needed, then train.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Best parameters provided by user\n",
    "best_params = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
    "\n",
    "# 1) Try to obtain data from notebook globals\n",
    "X_tr = None; X_te = None; y_tr = None; y_te = None\n",
    "\n",
    "if 'X_train_fs' in globals() and globals().get('X_train_fs') is not None:\n",
    "    X_tr = globals()['X_train_fs']\n",
    "    print('Using X_train_fs from notebook globals (selected features).')\n",
    "elif 'X_train_model' in globals() and globals().get('X_train_model') is not None:\n",
    "    X_tr = globals()['X_train_model']\n",
    "    print('Using X_train_model from notebook globals.')\n",
    "elif 'X_train_proc' in globals() and globals().get('X_train_proc') is not None:\n",
    "    X_tr = globals()['X_train_proc']\n",
    "    print('Using X_train_proc from notebook globals.')\n",
    "\n",
    "if 'X_test_fs' in globals() and globals().get('X_test_fs') is not None:\n",
    "    X_te = globals()['X_test_fs']\n",
    "elif 'X_test_model' in globals() and globals().get('X_test_model') is not None:\n",
    "    X_te = globals()['X_test_model']\n",
    "elif 'X_test_proc' in globals() and globals().get('X_test_proc') is not None:\n",
    "    X_te = globals()['X_test_proc']\n",
    "\n",
    "if 'y_train_model' in globals():\n",
    "    y_tr = globals()['y_train_model']\n",
    "elif 'y_train' in globals():\n",
    "    y_tr = globals()['y_train']\n",
    "\n",
    "if 'y_test_model' in globals():\n",
    "    y_te = globals()['y_test_model']\n",
    "elif 'y_test' in globals():\n",
    "    y_te = globals()['y_test']\n",
    "\n",
    "# 2) If missing, load dataset and prepare a minimal train/test\n",
    "if X_tr is None or y_tr is None:\n",
    "    print('Train globals not found — loading dataset and preparing train/test split from ../data/dataset_final_processed.csv')\n",
    "    df = pd.read_csv('../data/dataset_final_processed.csv')\n",
    "\n",
    "    # Drop any leakage column 'grav' (user requested)\n",
    "    if 'grav' in df.columns:\n",
    "        print(\"Dropping feature 'grav' from loaded dataset (user requested).\")\n",
    "        df = df.drop(columns=['grav'])\n",
    "\n",
    "    # Ensure target exists\n",
    "    if 'grave' not in df.columns:\n",
    "        raise RuntimeError(\"The loaded dataset does not contain a 'grave' target column.\")\n",
    "\n",
    "    # Build X, y\n",
    "    y = df['grave'].copy()\n",
    "    X = df.drop(columns=['grave'])\n",
    "\n",
    "    # Minimal cleaning: coerce object columns to numeric when possible\n",
    "    non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    dropped_object_cols = []\n",
    "    for col in non_numeric:\n",
    "        conv = pd.to_numeric(X[col].astype(str).str.replace(r'[()\\s]', '', regex=True).str.replace(',', '.', regex=False), errors='coerce')\n",
    "        frac_valid = conv.notna().mean()\n",
    "        if frac_valid >= 0.8:\n",
    "            X[col] = conv\n",
    "        else:\n",
    "            # drop unreliable object columns\n",
    "            dropped_object_cols.append(col)\n",
    "    if dropped_object_cols:\n",
    "        print('Dropped non-convertible object columns:', dropped_object_cols)\n",
    "        X.drop(columns=dropped_object_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Stratified split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "    print(f'Prepared train/test shapes: X_tr={X_tr.shape}, X_te={X_te.shape}')\n",
    "\n",
    "# 3) Ensure features do not contain columns entirely NaN in train\n",
    "all_nan_cols = []\n",
    "for c in X_tr.columns:\n",
    "    try:\n",
    "        if pd.isna(X_tr[c]).all():\n",
    "            all_nan_cols.append(c)\n",
    "    except Exception:\n",
    "        # ignore columns that cannot be checked\n",
    "        continue\n",
    "if all_nan_cols:\n",
    "    print('Dropping columns entirely NaN in train:', all_nan_cols)\n",
    "    X_tr = X_tr.drop(columns=all_nan_cols, errors='ignore')\n",
    "    if X_te is not None:\n",
    "        X_te = X_te.drop(columns=all_nan_cols, errors='ignore')\n",
    "\n",
    "# 4) Impute remaining NaNs with median (fit on train)\n",
    "numeric_cols = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) == 0:\n",
    "    raise RuntimeError('No numeric features available after cleaning — cannot train XGBoost.')\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_tr_imp = pd.DataFrame(imputer.fit_transform(X_tr[numeric_cols]), columns=numeric_cols, index=X_tr.index)\n",
    "X_te_imp = None\n",
    "if X_te is not None:\n",
    "    # align test to numeric_cols that exist\n",
    "    common = [c for c in numeric_cols if c in X_te.columns]\n",
    "    X_te_imp_values = imputer.transform(X_te[common])\n",
    "    X_te_imp = pd.DataFrame(X_te_imp_values, columns=list(common), index=X_te.index)\n",
    "\n",
    "# 5) Feature selection: use existing selected_features_kbest if available, else compute SelectKBest(k=8)\n",
    "if 'selected_features_kbest' in globals() and globals().get('selected_features_kbest'):\n",
    "    selected = [c for c in globals()['selected_features_kbest'] if c in X_tr_imp.columns]\n",
    "    print('Using selected_features_kbest from notebook globals:', selected)\n",
    "else:\n",
    "    k = min(8, X_tr_imp.shape[1])\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    selector.fit(X_tr_imp, y_tr)\n",
    "    mask = selector.get_support()\n",
    "    selected = list(X_tr_imp.columns[mask])\n",
    "    print(f'SelectKBest selected {len(selected)} features (top {k}):', selected)\n",
    "\n",
    "# Build final train/test feature matrices for modeling\n",
    "X_train_final = X_tr_imp.loc[:, selected].copy()\n",
    "X_test_final = X_te_imp.loc[:, selected].copy() if X_te_imp is not None else None\n",
    "\n",
    "print('Final modeling shapes -> X_train_final:', X_train_final.shape, 'X_test_final:', None if X_test_final is None else X_test_final.shape)\n",
    "\n",
    "# 6) Train XGBoost with the provided best params\n",
    "print('Training XGBoost with params:', best_params)\n",
    "xgb_final = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, verbosity=1)\n",
    "\n",
    "xgb_final.fit(X_train_final, y_tr)\n",
    "\n",
    "# 7) Evaluate\n",
    "if 'evaluate_model' in globals():\n",
    "    print('\\nEvaluating using notebook evaluate_model helper:')\n",
    "    globals()['evaluate_model']('XGBoost (final)', xgb_final, X_train_final, X_test_final if X_test_final is not None else X_train_final, y_tr, y_te if y_te is not None else y_tr)\n",
    "else:\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    if X_test_final is None or y_te is None:\n",
    "        print('No test set available; showing training metrics only.')\n",
    "        y_pred_tr = xgb_final.predict(X_train_final)\n",
    "        print('Train Accuracy:', accuracy_score(y_tr, y_pred_tr))\n",
    "        print('Train F1:', f1_score(y_tr, y_pred_tr))\n",
    "    else:\n",
    "        y_pred = xgb_final.predict(X_test_final)\n",
    "        print('Test Accuracy:', accuracy_score(y_te, y_pred))\n",
    "        print('Test F1:', f1_score(y_te, y_pred))\n",
    "        print('Precision:', precision_score(y_te, y_pred))\n",
    "        print('Recall:', recall_score(y_te, y_pred))\n",
    "\n",
    "# 8) Expose final model and metadata\n",
    "globals().update({'xgb_final_model': xgb_final, 'xgb_final_params': best_params, 'xgb_final_features': selected})\n",
    "print('\\nSaved final model as `xgb_final_model` and selected features as `xgb_final_features`.')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train globals not found — loading dataset and preparing train/test split from ../data/dataset_final_processed.csv\n",
      "Prepared train/test shapes: X_tr=(44241, 30), X_te=(11061, 30)\n",
      "Dropping columns entirely NaN in train: ['lat', 'long', 'v2', 'heure']\n",
      "SelectKBest selected 8 features (top 8): ['agg', 'col', 'nb_usagers', 'nb_vehicules', 'v1', 'plan', 'situ', 'vma']\n",
      "Final modeling shapes -> X_train_final: (44241, 8) X_test_final: (11061, 8)\n",
      "Training XGBoost with params: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Test Accuracy: 0.8944941687008408\n",
      "Test F1: 0.5471478463329453\n",
      "Precision: 0.6904995102840352\n",
      "Recall: 0.4530848329048843\n",
      "\n",
      "Saved final model as `xgb_final_model` and selected features as `xgb_final_features`.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "46d6089eac7e9c14",
   "metadata": {},
   "source": [
    "# Cross-validated recall optimization and final training"
   ]
  },
  {
   "cell_type": "code",
   "id": "df332f977a11f433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T11:04:26.393267Z",
     "iopub.status.busy": "2025-12-28T11:04:26.393189Z",
     "iopub.status.idle": "2025-12-28T11:04:31.283560Z",
     "shell.execute_reply": "2025-12-28T11:04:31.281960Z"
    },
    "ExecuteTime": {
     "end_time": "2025-12-28T11:07:03.740964Z",
     "start_time": "2025-12-28T11:06:57.892910Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, fbeta_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Require training matrices prepared by previous cell\n",
    "if 'X_train_final' not in globals() or 'y_tr' not in globals():\n",
    "    raise RuntimeError('X_train_final and y_tr must be available. Run the previous cell that prepares and trains the model first.')\n",
    "\n",
    "X_train_cv = globals()['X_train_final']\n",
    "y_train_cv = globals()['y_tr']\n",
    "X_test_cv = globals().get('X_test_final')\n",
    "y_test_cv = globals().get('y_te')\n",
    "\n",
    "print(f\"CV tuning on data: X_train_cv={X_train_cv.shape}, y_train_cv={len(y_train_cv)}\")\n",
    "\n",
    "# Base XGBoost with the user's best params as starting config\n",
    "base_params = globals().get('xgb_final_params', None)\n",
    "if base_params is None:\n",
    "    raise RuntimeError('Base XGBoost params not found in globals (`xgb_final_params`).')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "base_xgb = XGBClassifier(**base_params, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1, verbosity=0)\n",
    "\n",
    "# Candidate grid – we only vary scale_pos_weight and subsample slightly to improve recall\n",
    "# Compute a heuristic class ratio\n",
    "neg = int((y_train_cv == 0).sum())\n",
    "pos = int((y_train_cv == 1).sum())\n",
    "ratio = max(1, int(round(neg / pos)))\n",
    "param_grid = {\n",
    "    'scale_pos_weight': [1, ratio, max(1, ratio//2), ratio*2],\n",
    "    'subsample': [base_params.get('subsample', 0.8), 1.0]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(base_xgb, param_grid, scoring='recall', cv=cv, n_jobs=-1, verbose=1)\n",
    "print('Starting GridSearchCV (scoring=recall) over param grid:', param_grid)\n",
    "grid.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "print('\\nGridSearchCV complete.')\n",
    "print('Best params (recall):', grid.best_params_)\n",
    "print('Best CV recall:', grid.best_score_)\n",
    "\n",
    "best_xgb = grid.best_estimator_\n",
    "\n",
    "# Compute out-of-fold predicted probabilities for threshold search\n",
    "print('\\nComputing out-of-fold probabilities for threshold optimization (F2).')\n",
    "oof_proba = cross_val_predict(best_xgb, X_train_cv, y_train_cv, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "\n",
    "# Search thresholds to maximize F2 score (beta=2 emphasizes recall)\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "best_thr = 0.5\n",
    "best_f2 = -1.0\n",
    "from sklearn.metrics import fbeta_score\n",
    "for thr in thresholds:\n",
    "    preds = (oof_proba >= thr).astype(int)\n",
    "    try:\n",
    "        f2 = fbeta_score(y_train_cv, preds, beta=2)\n",
    "    except Exception:\n",
    "        f2 = 0.0\n",
    "    if f2 > best_f2:\n",
    "        best_f2 = f2\n",
    "        best_thr = thr\n",
    "\n",
    "print(f'Optimal threshold (F2 on OOF): {best_thr:.3f} with F2={best_f2:.4f}')\n",
    "\n",
    "# Fit final model on full training data\n",
    "print('\\nFitting best estimator on full training data...')\n",
    "best_xgb.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "# Evaluate on test if available\n",
    "if X_test_cv is not None and y_test_cv is not None:\n",
    "    y_proba_test = best_xgb.predict_proba(X_test_cv)[:, 1]\n",
    "    y_pred_test_default = best_xgb.predict(X_test_cv)\n",
    "    y_pred_test_thr = (y_proba_test >= best_thr).astype(int)\n",
    "\n",
    "    print('\\nEvaluation (default 0.5 threshold):')\n",
    "    print('Accuracy:', accuracy_score(y_test_cv, y_pred_test_default))\n",
    "    print('Recall:', recall_score(y_test_cv, y_pred_test_default))\n",
    "    print('Precision:', precision_score(y_test_cv, y_pred_test_default))\n",
    "    print('F1:', f1_score(y_test_cv, y_pred_test_default))\n",
    "    try:\n",
    "        print('ROC-AUC:', roc_auc_score(y_test_cv, y_proba_test))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print('\\nEvaluation (optimized threshold):')\n",
    "    print('Threshold:', best_thr)\n",
    "    print('Accuracy:', accuracy_score(y_test_cv, y_pred_test_thr))\n",
    "    print('Recall:', recall_score(y_test_cv, y_pred_test_thr))\n",
    "    print('Precision:', precision_score(y_test_cv, y_pred_test_thr))\n",
    "    print('F1:', f1_score(y_test_cv, y_pred_test_thr))\n",
    "    try:\n",
    "        print('ROC-AUC:', roc_auc_score(y_test_cv, y_proba_test))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print('\\nNo test set available for final evaluation. You can use the OOF F2/recall as an estimate.')\n",
    "    # Show OOF metrics at default and optimized threshold\n",
    "    oof_pred_def = (oof_proba >= 0.5).astype(int)\n",
    "    oof_pred_thr = (oof_proba >= best_thr).astype(int)\n",
    "    print('\\nOOF (default 0.5) Recall:', recall_score(y_train_cv, oof_pred_def), 'F1:', f1_score(y_train_cv, oof_pred_def))\n",
    "    print('OOF (opt thr) Recall:', recall_score(y_train_cv, oof_pred_thr), 'F2:', fbeta_score(y_train_cv, oof_pred_thr, beta=2))\n",
    "\n",
    "# Expose final tuned model and threshold\n",
    "globals().update({'xgb_cv_best': best_xgb, 'xgb_cv_best_params': grid.best_params_, 'xgb_cv_threshold': best_thr})\n",
    "print('\\nSaved tuned model as `xgb_cv_best` and threshold as `xgb_cv_threshold`.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV tuning on data: X_train_cv=(44241, 8), y_train_cv=44241\n",
      "Starting GridSearchCV (scoring=recall) over param grid: {'scale_pos_weight': [1, 6, 3, 12], 'subsample': [0.8, 1.0]}\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "GridSearchCV complete.\n",
      "Best params (recall): {'scale_pos_weight': 12, 'subsample': 1.0}\n",
      "Best CV recall: 0.8753210914397137\n",
      "\n",
      "Computing out-of-fold probabilities for threshold optimization (F2).\n",
      "Optimal threshold (F2 on OOF): 0.670 with F2=0.7073\n",
      "\n",
      "Fitting best estimator on full training data...\n",
      "\n",
      "Evaluation (default 0.5 threshold):\n",
      "Accuracy: 0.7799475635114366\n",
      "Recall: 0.8753213367609255\n",
      "Precision: 0.3781232648528595\n",
      "F1: 0.5281116711903838\n",
      "ROC-AUC: 0.8931362400252066\n",
      "\n",
      "Evaluation (optimized threshold):\n",
      "Threshold: 0.67\n",
      "Accuracy: 0.8277732573908326\n",
      "Recall: 0.8155526992287918\n",
      "Precision: 0.4395566331832352\n",
      "F1: 0.5712356515867657\n",
      "ROC-AUC: 0.8931362400252066\n",
      "\n",
      "Saved tuned model as `xgb_cv_best` and threshold as `xgb_cv_threshold`.\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
